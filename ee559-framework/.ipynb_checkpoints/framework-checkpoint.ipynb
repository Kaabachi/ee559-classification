{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x2605a952688>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import dlc_practical_prologue as prologue\n",
    "import math\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "'''\n",
    "class Module ( object ) :\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args)\n",
    "    def forward ( self , * input ) :\n",
    "        raise NotImplementedError\n",
    "    def backward ( self , * gradwrtoutput ) :\n",
    "        raise NotImplementedError\n",
    "    def param ( self ) :\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Linear(Module):\n",
    "    \n",
    "    '''\n",
    "    Applies a linear transformation to the incoming data: y = xA^T + b\n",
    "    \n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        bias: If set to ``False``, the layer will not learn an additive bias.\n",
    "            Default: ``True``\n",
    "\n",
    "    Attributes:\n",
    "        weight: the learnable weights of the module \n",
    "        bias:   the learnable bias of the module of shape \n",
    "        \n",
    "\n",
    "    '''\n",
    "    def __init__(self, in_features, out_features, bias = True):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        \n",
    "        # Initialize weight and bias\n",
    "        # according to https://discuss.pytorch.org/t/how-are-layer-weights-and-biases-initialized-by-default/13073\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = torch.empty(1,out_features)\n",
    "        self.weight = torch.empty(in_features,out_features)\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "        # cache the input whenever we go forward\n",
    "        self.x = 0\n",
    "        self.grad_weight = torch.empty(in_features,out_features)\n",
    "        self.grad_bias = torch.empty(1,out_features)\n",
    "            \n",
    "        \n",
    "    #Applies a linear transformation to the incoming data: y = xA^T + b   \n",
    "    def forward(self,input):\n",
    "        self.x = input\n",
    "        output = input.mm(self.weight)\n",
    "        if self.bias is not None:\n",
    "            output += self.bias\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def backward(self,input):\n",
    "        x = self.x\n",
    "        w = self.weight\n",
    "        b = self.bias\n",
    "        \n",
    "        # divide weigthts by batch size \n",
    "        # inspiration from https://stats.stackexchange.com/questions/183840/sum-or-average-of-gradients-in-mini-batch-gradient-decent\n",
    "        db = input.sum(0).div(input.size()[0])\n",
    "        dx = input.mm(w.t())\n",
    "        dw = x.t().mm(input).div(input.size()[0])\n",
    "        \n",
    "        \n",
    "\n",
    "        self.grad_bias += db\n",
    "        self.grad_weight += dw\n",
    "\n",
    "\n",
    "        return dx\n",
    "            \n",
    "\n",
    "    def param(self):\n",
    "        return[(self.weight,self.grad_weight),(self.bias,self.grad_bias)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Applies the element-wise function Rectified Linear Unit\n",
    "\n",
    "Args:\n",
    "    in_features: size of each input sample\n",
    "    out_features: size of each output sample\n",
    "    bias: If set to ``False``, the layer will not learn an additive bias.\n",
    "        Default: ``True``\n",
    "\n",
    "'''\n",
    "class ReLU(Module):\n",
    "    def __init_(self):\n",
    "        self.x = 0\n",
    "    def forward( self,  input ):\n",
    "        self.x = input\n",
    "        s1 = input.clamp(min=0)\n",
    "        return s1\n",
    "    def backward(self, input):\n",
    "        return (self.x>0).float()*input\n",
    "    \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Applies the element-wise function Hyperbolic Tangent\n",
    "\n",
    "'''\n",
    "# TODO change implementation\n",
    "class Tanh(Module):\n",
    "    def forward(self,input):\n",
    "        s1 = input.tanh()\n",
    "        return s1\n",
    "    def backward(self,input):\n",
    "        x1 = 4 * (torch.exp(input) + torch.exp(torch.mul(input,-1))).pow(-2)\n",
    "        return x1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A sequential container.\n",
    "Modules are added to it in the order they are passed in the constructor - in a list.\n",
    "\n",
    "'''\n",
    "class Sequential(Module):\n",
    "    def __init__(self, param ):\n",
    "        super().__init__()\n",
    "        self.model = (param)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        for _ in self.model:\n",
    "            x = _.forward(x)\n",
    "        return x\n",
    "    def backward(self,x):\n",
    "        for _ in reversed(self.model):\n",
    "            x = _.backward(x)\n",
    "        return x\n",
    "    \n",
    "    def param(self):\n",
    "        param_list = []\n",
    "        for module in self.model:\n",
    "            param_list.extend(module.param())\n",
    "\n",
    "        return param_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Returns mean square error loss\n",
    "'''\n",
    "#TODO change to MSE => mean instead of sum\n",
    "class MSELoss(Module):\n",
    "    def forward(self,v,t):\n",
    "        return (v-t).pow(2).sum(0).sum()\n",
    "    def dloss(self,v,t):\n",
    "        return (2*(v-t))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO change this\n",
    "class SGD(object):\n",
    "\n",
    "    def __init__(self, params, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.params = params\n",
    "\n",
    "\n",
    "    def step(self):\n",
    "\n",
    "        for i, (p, p_grad) in enumerate(self.params):\n",
    "            Vt = p_grad\n",
    "\n",
    "            # update parameter\n",
    "\n",
    "            p.add_(-self.lr*Vt)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "\n",
    "        for param in self.params:\n",
    "            param[1].zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO have a better disc set\n",
    "def generate_disc_set(nb):\n",
    "    train = torch.empty(nb,2).uniform_(-1,1)\n",
    "    target = (train.pow(2).sum(1)<torch.empty(nb).fill_(math.sqrt(2/math.pi))).long()\n",
    "    return train,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO change conversion\n",
    "def target_to_onehot(target):\n",
    "    res = torch.empty(target.size(0), 2).zero_()\n",
    "    res.scatter_(1, target.view(-1, 1), 1.0).mul(0.9)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO graph to see the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0167,  0.3364],\n",
       "        [ 0.3039,  0.9305],\n",
       "        [ 1.1378, -0.6998],\n",
       "        ...,\n",
       "        [ 1.4785,  1.5781],\n",
       "        [ 1.2005, -0.4963],\n",
       "        [-0.1300,  1.5770]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input,train_target = generate_disc_set(1000)\n",
    "test_input,test_target = generate_disc_set(1000)\n",
    "\n",
    "mu,std = train_input.mean(0), train_input.std(0)\n",
    "train_input.sub_(mu).div_(std)\n",
    "\n",
    "mu,std = test_input.mean(0), test_input.std(0)\n",
    "test_input.sub_(mu).div_(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = target_to_onehot(train_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target = target_to_onehot(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_epochs = 250\n",
    "def train_model(model,train_input,train_target):\n",
    "    criterion = MSELoss()\n",
    "    optimizer = SGD(model.param(),lr = 0.01)\n",
    "    for e in range(0,n_epochs):\n",
    "        for input, targets in zip(train_input.split(batch_size),train_target.split(batch_size)):\n",
    "            output = model(input)\n",
    "            loss = criterion(output,targets)\n",
    "            optimizer.zero_grad()\n",
    "            model.backward(criterion.dloss(output,targets))\n",
    "            optimizer.step()\n",
    "            if e%50==0 :\n",
    "                print('epoch: ',e,' loss: ',loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential((Linear(2,25),Tanh(),Linear(25,25),Tanh(),Linear(25,25),Tanh(),Linear(25,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  tensor(12.1567)\n",
      "epoch:  0  loss:  tensor(12.1539)\n",
      "epoch:  0  loss:  tensor(13.3545)\n",
      "epoch:  0  loss:  tensor(15.3937)\n",
      "epoch:  0  loss:  tensor(10.0684)\n",
      "epoch:  0  loss:  tensor(14.8465)\n",
      "epoch:  0  loss:  tensor(13.3533)\n",
      "epoch:  0  loss:  tensor(13.1386)\n",
      "epoch:  0  loss:  tensor(14.6776)\n",
      "epoch:  0  loss:  tensor(13.8126)\n",
      "epoch:  50  loss:  tensor(11.9288)\n",
      "epoch:  50  loss:  tensor(11.9749)\n",
      "epoch:  50  loss:  tensor(13.1088)\n",
      "epoch:  50  loss:  tensor(15.0257)\n",
      "epoch:  50  loss:  tensor(9.8586)\n",
      "epoch:  50  loss:  tensor(14.5178)\n",
      "epoch:  50  loss:  tensor(13.0933)\n",
      "epoch:  50  loss:  tensor(12.8365)\n",
      "epoch:  50  loss:  tensor(14.4872)\n",
      "epoch:  50  loss:  tensor(13.5364)\n",
      "epoch:  100  loss:  tensor(11.6613)\n",
      "epoch:  100  loss:  tensor(11.7632)\n",
      "epoch:  100  loss:  tensor(12.8244)\n",
      "epoch:  100  loss:  tensor(14.6032)\n",
      "epoch:  100  loss:  tensor(9.6106)\n",
      "epoch:  100  loss:  tensor(14.1451)\n",
      "epoch:  100  loss:  tensor(12.7881)\n",
      "epoch:  100  loss:  tensor(12.5033)\n",
      "epoch:  100  loss:  tensor(14.2565)\n",
      "epoch:  100  loss:  tensor(13.2139)\n",
      "epoch:  150  loss:  tensor(11.3487)\n",
      "epoch:  150  loss:  tensor(11.5143)\n",
      "epoch:  150  loss:  tensor(12.4934)\n",
      "epoch:  150  loss:  tensor(14.1215)\n",
      "epoch:  150  loss:  tensor(9.3188)\n",
      "epoch:  150  loss:  tensor(13.7261)\n",
      "epoch:  150  loss:  tensor(12.4335)\n",
      "epoch:  150  loss:  tensor(12.1358)\n",
      "epoch:  150  loss:  tensor(13.9792)\n",
      "epoch:  150  loss:  tensor(12.8395)\n",
      "epoch:  200  loss:  tensor(10.9867)\n",
      "epoch:  200  loss:  tensor(11.2258)\n",
      "epoch:  200  loss:  tensor(12.1067)\n",
      "epoch:  200  loss:  tensor(13.5782)\n",
      "epoch:  200  loss:  tensor(8.9783)\n",
      "epoch:  200  loss:  tensor(13.2617)\n",
      "epoch:  200  loss:  tensor(12.0275)\n",
      "epoch:  200  loss:  tensor(11.7310)\n",
      "epoch:  200  loss:  tensor(13.6510)\n",
      "epoch:  200  loss:  tensor(12.4112)\n"
     ]
    }
   ],
   "source": [
    "train_model(model,train_input,train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "            \n",
    "def compute_nb_errors(model,data_input,data_target):\n",
    "    nb_errors = 0\n",
    "    for input,targets in zip(data_input.split(batch_size),data_target.split(batch_size)):\n",
    "        output = model(input)\n",
    "        _,predicted_classes = torch.max(output,1)\n",
    "        for i in range(0,output.size(0)):\n",
    "            if(targets[i][predicted_classes[i]]!=1):\n",
    "                nb_errors = nb_errors+1\n",
    "                \n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_nb_errors(model,train_input,train_target)/1000 * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_nb_errors(model,test_input,test_target)/1000 * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
